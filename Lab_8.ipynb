{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5kCrTbhUjpo",
        "outputId": "eaca25ac-b7ab-4826-de7f-12e7e36865ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A2 AND Step epochs: 130\n",
            "A3 Step epochs: 130\n",
            "A3 Bipolar epochs: 1000\n",
            "A3 Sigmoid epochs: 130\n",
            "A3 ReLU epochs: 124\n",
            "A4 lr vs epochs: {0.1: 68, 0.2: 37, 0.30000000000000004: 23, 0.4: 23, 0.5: 19, 0.6000000000000001: 19, 0.7000000000000001: 15, 0.8: 14, 0.9: 13, 1.0: 12}\n",
            "A5 XOR Step epochs: 1000\n",
            "A6 accuracy: 1.0\n",
            "A7 pseudo-inverse weights: [ 0.6        -0.09436819  0.21713405 -0.01342766  0.23416864]\n",
            "A8 AND backprop epochs: 1000\n",
            "A9 XOR backprop epochs: 1000\n",
            "A10 two-output epochs: 1000\n",
            "A11 AND preds: [0 0 0 1]\n",
            "A11 XOR preds: [0 1 1 0]\n",
            "A12 train acc: 0.9425255595468361 test acc: 0.945303867403315\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import pinv\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import io\n",
        "\n",
        "# -------------------------\n",
        "# A1: Units\n",
        "# -------------------------\n",
        "def summation_unit(x, w, b): return np.dot(x, w) + b\n",
        "def step(x): return (np.array(x) >= 0).astype(float)\n",
        "def bipolar_step(x): return np.where(np.array(x) >= 0, 1.0, -1.0)\n",
        "def sigmoid(x): return 1 / (1 + np.exp(-np.array(x)))\n",
        "def tanh(x): return np.tanh(x)\n",
        "def relu(x): return np.maximum(0, np.array(x))\n",
        "def leaky_relu(x, alpha=0.01): return np.where(np.array(x) > 0, x, alpha*np.array(x))\n",
        "def comparator(y_true, y_pred): return y_true - y_pred\n",
        "\n",
        "# -------------------------\n",
        "# A2: Perceptron Trainer\n",
        "# -------------------------\n",
        "def perceptron_train(X, y, activation_fn, lr=0.05, w_init=[10,0.2,-0.75],\n",
        "                     tol=0.002, max_epochs=1000):\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    b, w = w_init[0], np.array(w_init[1:])\n",
        "    errors=[]\n",
        "    for epoch in range(max_epochs):\n",
        "        sse=0\n",
        "        for i in range(len(X)):\n",
        "            z = summation_unit(X[i],w,b)\n",
        "            y_pred = float(np.ravel(activation_fn(z))[0])\n",
        "            err = y[i]-y_pred\n",
        "            w += lr*err*X[i]\n",
        "            b += lr*err\n",
        "            sse += err**2\n",
        "        errors.append(sse)\n",
        "        if sse<=tol: break\n",
        "    return w,b,errors,epoch+1\n",
        "\n",
        "def plot_errors(errors,title):\n",
        "    plt.figure()\n",
        "    plt.plot(errors); plt.title(title)\n",
        "    plt.xlabel(\"Epochs\"); plt.ylabel(\"SSE\"); plt.grid(True);\n",
        "    plt.savefig(f'{title.replace(\" \", \"_\")}.png')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# A3: Compare activations\n",
        "# -------------------------\n",
        "def compare_activations(X,y,activations):\n",
        "    results={}\n",
        "    for name,fn in activations:\n",
        "        w,b,e,ep=perceptron_train(X,y,fn)\n",
        "        results[name]=(w,b,e,ep)\n",
        "    return results\n",
        "\n",
        "# -------------------------\n",
        "# A4: Vary learning rate\n",
        "# -------------------------\n",
        "def vary_lr(X,y,activation_fn,lrs):\n",
        "    out={}\n",
        "    for lr in lrs:\n",
        "        _,_,_,ep=perceptron_train(X,y,activation_fn,lr=lr)\n",
        "        out[lr]=ep\n",
        "    return out\n",
        "\n",
        "# -------------------------\n",
        "# A6: Customer dataset (from lab sheet)\n",
        "# -------------------------\n",
        "def get_customer_data():\n",
        "    data = {\n",
        "        \"Candies\":[20,16,27,19,24,22,15,18,21,16],\n",
        "        \"Mangoes\":[6,3,6,1,4,1,4,4,1,2],\n",
        "        \"Milk\":[2,6,2,2,2,5,2,2,4,4],\n",
        "        \"Payment\":[386,289,393,110,280,167,271,274,148,198],\n",
        "        \"High?\":[\"Yes\",\"Yes\",\"Yes\",\"No\",\"Yes\",\"No\",\"Yes\",\"Yes\",\"No\",\"No\"]\n",
        "    }\n",
        "    df=pd.DataFrame(data)\n",
        "    y=(df[\"High?\"]==\"Yes\").astype(int).values\n",
        "    X=df.drop(columns=[\"High?\"]).values\n",
        "    X=StandardScaler().fit_transform(X)\n",
        "    return X,y\n",
        "\n",
        "def sigmoid_train(X,y,lr=0.05,tol=0.002,max_epochs=10000):\n",
        "    w=np.zeros(X.shape[1]); b=0; errors=[]\n",
        "    for epoch in range(max_epochs):\n",
        "        net=X.dot(w)+b; yhat=sigmoid(net)\n",
        "        err=y-yhat; sse=np.sum(err**2); errors.append(sse)\n",
        "        if sse<=tol: break\n",
        "        grad=X.T.dot(yhat-y)/len(X); gradb=np.mean(yhat-y)\n",
        "        w-=lr*grad; b-=lr*gradb\n",
        "    return w,b,errors,epoch+1\n",
        "\n",
        "# -------------------------\n",
        "# A7: Pseudo-inverse\n",
        "# -------------------------\n",
        "def pseudo_inverse(X,y):\n",
        "    X_aug=np.hstack([np.ones((X.shape[0],1)),X])\n",
        "    return pinv(X_aug).dot(y)\n",
        "\n",
        "# -------------------------\n",
        "# A8â€“A10: Backprop\n",
        "# -------------------------\n",
        "def init_params(sizes):\n",
        "    rng=np.random.default_rng(42)\n",
        "    params=[]\n",
        "    for i in range(len(sizes)-1):\n",
        "        W=rng.normal(scale=0.5,size=(sizes[i+1],sizes[i]))\n",
        "        b=np.zeros((sizes[i+1],))\n",
        "        params.append([W,b])\n",
        "    return params\n",
        "\n",
        "def forward(X,params):\n",
        "    a=X; acts=[a]; zs=[]\n",
        "    for W,b in params:\n",
        "        z=a.dot(W.T)+b; zs.append(z)\n",
        "        a=sigmoid(z); acts.append(a)\n",
        "    return zs,acts\n",
        "\n",
        "def backprop(X,y,params,lr=0.05):\n",
        "    n=len(X); zs,acts=forward(X,params)\n",
        "    delta=(acts[-1]-y)*acts[-1]*(1-acts[-1]); deltas=[delta]\n",
        "    for l in range(len(params)-1,0,-1):\n",
        "        Wn,_=params[l]\n",
        "        delta=(deltas[0].dot(Wn))*acts[l]*(1-acts[l])\n",
        "        deltas.insert(0,delta)\n",
        "    for i,(W,b) in enumerate(params):\n",
        "        gradW=(deltas[i].T.dot(acts[i]))/n; gradb=np.mean(deltas[i],axis=0)\n",
        "        params[i][0]-=lr*gradW; params[i][1]-=lr*gradb\n",
        "    return np.sum((y-acts[-1])**2)\n",
        "\n",
        "def train_backprop(X,y,sizes,lr=0.05,tol=0.002,max_epochs=1000):\n",
        "    params=init_params(sizes); errors=[]\n",
        "    for ep in range(max_epochs):\n",
        "        sse=backprop(X,y,params,lr); errors.append(sse)\n",
        "        if sse<=tol: break\n",
        "    return params,errors,ep+1\n",
        "\n",
        "# -------------------------\n",
        "# A11 & A12: sklearn MLP\n",
        "# -------------------------\n",
        "def sklearn_mlp(X,y,hidden=(4,),activation='logistic'):\n",
        "    clf=MLPClassifier(hidden_layer_sizes=hidden,activation=activation,\n",
        "                        learning_rate_init=0.05,max_iter=2000,random_state=42)\n",
        "    clf.fit(X,y); return clf\n",
        "\n",
        "# -------------------------\n",
        "# MAIN\n",
        "# -------------------------\n",
        "def main():\n",
        "    X_and=np.array([[0,0],[0,1],[1,0],[1,1]]); y_and=np.array([0,0,0,1])\n",
        "    X_xor=X_and; y_xor=np.array([0,1,1,0])\n",
        "\n",
        "    # A2\n",
        "    w,b,errs,ep=perceptron_train(X_and,y_and,step)\n",
        "    print(f\"A2 AND Step epochs: {ep}\"); plot_errors(errs,\"A2 AND Step\")\n",
        "\n",
        "    # A3\n",
        "    acts=[(\"Step\",step),(\"Bipolar\",bipolar_step),\n",
        "          (\"Sigmoid\",lambda x:(sigmoid(x)>=0.5).astype(float)),\n",
        "          (\"ReLU\",lambda x:(relu(x)>=0.5).astype(float))]\n",
        "    res=compare_activations(X_and,y_and,acts)\n",
        "    for k,(w,b,e,ep) in res.items(): print(f\"A3 {k} epochs: {ep}\")\n",
        "\n",
        "    # A4\n",
        "    lr_epochs=vary_lr(X_and,y_and,step,[0.1*i for i in range(1,11)])\n",
        "    print(f\"A4 lr vs epochs: {lr_epochs}\")\n",
        "\n",
        "    # A5\n",
        "    w,b,errs,ep=perceptron_train(X_xor,y_xor,step)\n",
        "    print(f\"A5 XOR Step epochs: {ep}\")\n",
        "\n",
        "    # A6\n",
        "    Xc,yc=get_customer_data()\n",
        "    w,b,errs,ep=sigmoid_train(Xc,yc)\n",
        "    preds=(sigmoid(Xc.dot(w)+b)>=0.5).astype(int)\n",
        "    print(f\"A6 accuracy: {np.mean(preds==yc)}\")\n",
        "\n",
        "    # A7\n",
        "    W=pseudo_inverse(Xc,yc); print(f\"A7 pseudo-inverse weights: {W}\")\n",
        "\n",
        "    # A8 AND backprop\n",
        "    _,errs,ep=train_backprop(X_and,y_and.reshape(-1,1),(2,2,1))\n",
        "    print(f\"A8 AND backprop epochs: {ep}\")\n",
        "\n",
        "    # A9 XOR backprop\n",
        "    _,errs,ep=train_backprop(X_xor,y_xor.reshape(-1,1),(2,4,1))\n",
        "    print(f\"A9 XOR backprop epochs: {ep}\")\n",
        "\n",
        "    # A10 two-output AND\n",
        "    y2=np.array([[1,0] if v==0 else [0,1] for v in y_and])\n",
        "    _,errs,ep=train_backprop(X_and,y2,(2,3,2))\n",
        "    print(f\"A10 two-output epochs: {ep}\")\n",
        "\n",
        "    # A11 sklearn MLP on gates\n",
        "    print(f\"A11 AND preds: {sklearn_mlp(X_and,y_and).predict(X_and)}\")\n",
        "    print(f\"A11 XOR preds: {sklearn_mlp(X_xor,y_xor).predict(X_xor)}\")\n",
        "\n",
        "    # A12: sklearn MLP on merged USGS dataset\n",
        "    def load_usgs_data(filepath, value_name):\n",
        "        \"\"\"Loads and preprocesses a single USGS data file.\"\"\"\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"Error: File '{filepath}' not found.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Pre-process the file to remove surrounding quotes from each line\n",
        "        cleaned_lines = []\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f:\n",
        "                # Strip whitespace and the quote characters\n",
        "                cleaned_line = line.strip().strip('\"')\n",
        "                if cleaned_line and not cleaned_line.startswith('#'):\n",
        "                    cleaned_lines.append(cleaned_line)\n",
        "\n",
        "        if not cleaned_lines:\n",
        "            print(f\"Warning: File '{filepath}' is empty or only contains comments after cleaning.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Join cleaned lines into a single string and wrap in a StringIO object\n",
        "        cleaned_data_string = \"\\n\".join(cleaned_lines)\n",
        "        data_io = io.StringIO(cleaned_data_string)\n",
        "\n",
        "        column_names = ['agency', 'site_no', 'datetime', 'tz', 'value', 'code']\n",
        "        # Now read the cleaned, in-memory data\n",
        "        df = pd.read_csv(data_io, sep='\\t', header=None, names=column_names)\n",
        "\n",
        "        # Now the standard filtering should work perfectly\n",
        "        df = df[df['agency'] == 'USGS'].copy()\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"Warning: No data with agency 'USGS' found in {filepath} after cleaning.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
        "        df[value_name] = pd.to_numeric(df['value'], errors='coerce')\n",
        "\n",
        "        # Handle duplicate timestamps by averaging them\n",
        "        df = df.groupby('datetime').mean(numeric_only=True)\n",
        "\n",
        "        return df[[value_name]]\n",
        "\n",
        "    # Load and merge the four datasets\n",
        "    df_gage = load_usgs_data('gage_height.csv', 'gage_height')\n",
        "    df_precip = load_usgs_data('precipitation.csv', 'precipitation')\n",
        "    df_res = load_usgs_data('reservoir_storage.csv', 'reservoir_storage')\n",
        "    df_stream = load_usgs_data('stream_flow.csv', 'stream_flow')\n",
        "\n",
        "    df = pd.concat([df_gage, df_precip, df_res, df_stream], axis=1)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"Stopping execution: The merged dataframe is empty after loading the files.\")\n",
        "        return\n",
        "\n",
        "    # Define target and features\n",
        "    target_col = 'stream_flow'\n",
        "    df['target'] = (df[target_col] > df[target_col].median()).astype(int)\n",
        "    df.dropna(subset=['target'], inplace=True)\n",
        "\n",
        "    X = df.drop(columns=[target_col, 'target'])\n",
        "    y = df['target'].values\n",
        "    X = X.fillna(X.mean()).values\n",
        "\n",
        "    if X.shape[0] == 0:\n",
        "        print(\"Stopping execution: Feature set is empty after processing.\")\n",
        "        return\n",
        "\n",
        "    # Scale, split, train, and evaluate\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    clf = sklearn_mlp(Xtr, ytr, hidden=(8,), activation='relu')\n",
        "    print(f\"A12 train acc: {clf.score(Xtr, ytr)} test acc: {clf.score(Xte, yte)}\")\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ]
    }
  ]
}