{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwOq2ztZIWAP",
        "outputId": "f166b68b-fbac-4c7c-f0c6-10470ce88b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================\n",
            "      MODEL EVALUATION RESULTS      \n",
            "====================================\n",
            "âœ… Accuracy: 0.9569\n",
            "\n",
            "ðŸ“‹ Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Low Flow       1.00      0.92      0.96       909\n",
            "   High Flow       0.92      1.00      0.96       901\n",
            "\n",
            "    accuracy                           0.96      1810\n",
            "   macro avg       0.96      0.96      0.96      1810\n",
            "weighted avg       0.96      0.96      0.96      1810\n",
            "\n",
            "\n",
            "====================================\n",
            "         LIME EXPLANATION         \n",
            "====================================\n",
            "ðŸ”Ž LIME Explanation for Test Sample #0\n",
            "\n",
            "--- True Feature Values ---\n",
            "  gage_height: 20.23\n",
            "  precipitation: 0.0\n",
            "  reservoir_storage: 34.38\n",
            "\n",
            "--- Predicted Probabilities ---\n",
            "  P(class=Low Flow) = 0.064\n",
            "  P(class=High Flow) = 0.936\n",
            "\n",
            "--- Feature Contributions (Top) ---\n",
            "  34.37 < reservoir_storage <= 34.87 --> Contribution: 0.426\n",
            "  precipitation <= 0.00     --> Contribution: -0.130\n",
            "  20.19 < gage_height <= 20.23 --> Contribution: -0.014\n",
            "====================================\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# Lab 09 â€“ Stacking + Pipeline + LIME (Corrected)\n",
        "# ==============================\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "from functools import reduce\n",
        "\n",
        "# Suppress warnings for clean output\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Function: Load Data\n",
        "# ------------------------------\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loads, merges, and preprocesses the project-specific datasets.\n",
        "    This version reads gage height, precipitation, reservoir storage, and stream flow data,\n",
        "    merges them by datetime, and creates a binary classification target for stream flow.\n",
        "    \"\"\"\n",
        "\n",
        "    # Helper function to parse the specific USGS CSV format\n",
        "    def parse_usgs_file(filepath, value_name):\n",
        "        try:\n",
        "            # The files have a format where each line is a single quoted string containing tabs.\n",
        "            # 1. Read the file as a single column, skipping commented header lines.\n",
        "            df = pd.read_csv(filepath, header=None, comment='#')\n",
        "\n",
        "            if df.empty:\n",
        "                return pd.DataFrame({'datetime': [], value_name: []})\n",
        "\n",
        "            # 2. Strip quotes and split the single column into multiple columns by tab.\n",
        "            data = df[0].str.strip('\"').str.split('\\t', expand=True)\n",
        "\n",
        "            # 3. Assign column names based on the file format description.\n",
        "            column_names = ['agency_cd', 'site_no', 'datetime_str', 'tz_cd', 'value', 'prov_qual_cd']\n",
        "            data.columns = column_names[:data.shape[1]]\n",
        "\n",
        "            # 4. Keep relevant columns, convert types, and rename the value column.\n",
        "            df_clean = data[['datetime_str', 'value']].copy()\n",
        "            df_clean.rename(columns={'datetime_str': 'datetime', 'value': value_name}, inplace=True)\n",
        "\n",
        "            df_clean['datetime'] = pd.to_datetime(df_clean['datetime'])\n",
        "            df_clean[value_name] = pd.to_numeric(df_clean[value_name], errors='coerce')\n",
        "\n",
        "            return df_clean\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: The file '{filepath}' was not found.\")\n",
        "            return None\n",
        "\n",
        "    # File paths for the datasets\n",
        "    files = {\n",
        "        'gage_height': 'gage_height.csv',\n",
        "        'precipitation': 'precipitation.csv',\n",
        "        'reservoir_storage': 'reservoir_storage.csv',\n",
        "        'stream_flow': 'stream_flow.csv'\n",
        "    }\n",
        "\n",
        "    # Load and parse each file\n",
        "    df_gage = parse_usgs_file(files['gage_height'], 'gage_height')\n",
        "    df_precip = parse_usgs_file(files['precipitation'], 'precipitation')\n",
        "    df_res = parse_usgs_file(files['reservoir_storage'], 'reservoir_storage')\n",
        "    df_flow = parse_usgs_file(files['stream_flow'], 'stream_flow')\n",
        "\n",
        "    # Filter out any dataframes that failed to load\n",
        "    data_frames = [df for df in [df_gage, df_precip, df_res, df_flow] if df is not None]\n",
        "\n",
        "    if len(data_frames) < 4:\n",
        "        print(\"Could not load all necessary data files. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Merge the dataframes on the 'datetime' column\n",
        "    df_merged = reduce(lambda left, right: pd.merge(left, right, on='datetime', how='inner'), data_frames)\n",
        "\n",
        "    # Drop any rows with missing values\n",
        "    df_merged.dropna(inplace=True)\n",
        "\n",
        "    # Define features (X) and the target variable (y)\n",
        "    feature_names = ['gage_height', 'precipitation', 'reservoir_storage']\n",
        "    X = df_merged[feature_names]\n",
        "\n",
        "    # Create a binary classification target from 'stream_flow'\n",
        "    # \"High flow\" is anything above the median stream flow.\n",
        "    flow_threshold = df_merged['stream_flow'].median()\n",
        "    y = (df_merged['stream_flow'] > flow_threshold).astype(int)\n",
        "\n",
        "    # Define class names for LIME explainer\n",
        "    class_names = ['Low Flow', 'High Flow']\n",
        "\n",
        "    return X, y, feature_names, class_names\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Function: Build Stacking Model\n",
        "# ------------------------------\n",
        "def build_stacking_classifier():\n",
        "    \"\"\"Builds a Stacking Classifier with base models and a meta-model.\"\"\"\n",
        "    base_models = [\n",
        "        (\"rf\", RandomForestClassifier(n_estimators=50, random_state=42)),\n",
        "        (\"gb\", GradientBoostingClassifier(n_estimators=50, random_state=42))\n",
        "    ]\n",
        "    meta_model = LogisticRegression()\n",
        "\n",
        "    return StackingClassifier(\n",
        "        estimators=base_models,\n",
        "        final_estimator=meta_model,\n",
        "        passthrough=True\n",
        "    )\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Function: Build Pipeline\n",
        "# ------------------------------\n",
        "def build_pipeline(model):\n",
        "    \"\"\"Builds a scikit-learn pipeline with a scaler and a model.\"\"\"\n",
        "    return Pipeline(steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", model)\n",
        "    ])\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Function: Train & Evaluate\n",
        "# ------------------------------\n",
        "def train_and_evaluate(pipeline, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Fits the pipeline and returns accuracy, report, and the fitted pipeline.\"\"\"\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    preds = pipeline.predict(X_test)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    report = classification_report(y_test, preds, target_names=['Low Flow', 'High Flow'])\n",
        "    return acc, report, pipeline\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Function: Explain with LIME\n",
        "# ------------------------------\n",
        "def explain_with_lime(pipeline, X_train, X_test, feature_names, class_names):\n",
        "    \"\"\"\n",
        "    Generates a LIME explanation for a single test sample.\n",
        "    This function now returns the explanation details instead of printing them.\n",
        "    \"\"\"\n",
        "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "        training_data=X_train.values,\n",
        "        feature_names=feature_names,\n",
        "        class_names=class_names,\n",
        "        mode=\"classification\"\n",
        "    )\n",
        "\n",
        "    sample_index = 0\n",
        "    sample_to_explain = X_test.iloc[sample_index]\n",
        "\n",
        "    explanation = explainer.explain_instance(\n",
        "        data_row=sample_to_explain.values,\n",
        "        predict_fn=pipeline.predict_proba\n",
        "    )\n",
        "\n",
        "    explanation_details = {\n",
        "        \"sample_index\": sample_index,\n",
        "        \"true_features\": sample_to_explain.to_dict(),\n",
        "        \"predicted_probabilities\": pipeline.predict_proba([sample_to_explain])[0],\n",
        "        \"explanation_list\": explanation.as_list()\n",
        "    }\n",
        "\n",
        "    return explanation_details\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Main Program\n",
        "# ------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    X, y, feature_names, class_names = load_data()\n",
        "\n",
        "    if X.empty:\n",
        "        print(\"Data loading resulted in an empty dataset. Cannot proceed.\")\n",
        "    else:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        stacking_model = build_stacking_classifier()\n",
        "        pipeline = build_pipeline(stacking_model)\n",
        "\n",
        "        acc, report, fitted_pipeline = train_and_evaluate(pipeline, X_train, X_test, y_train, y_test)\n",
        "\n",
        "        print(\"====================================\")\n",
        "        print(\"      MODEL EVALUATION RESULTS      \")\n",
        "        print(\"====================================\")\n",
        "        print(f\"âœ… Accuracy: {acc:.4f}\")\n",
        "        print(\"\\nðŸ“‹ Classification Report:\\n\", report)\n",
        "\n",
        "        lime_explanation = explain_with_lime(fitted_pipeline, X_train, X_test, feature_names, class_names)\n",
        "\n",
        "        print(\"\\n====================================\")\n",
        "        print(\"         LIME EXPLANATION         \")\n",
        "        print(\"====================================\")\n",
        "        print(f\"ðŸ”Ž LIME Explanation for Test Sample #{lime_explanation['sample_index']}\")\n",
        "        print(\"\\n--- True Feature Values ---\")\n",
        "        for feature, value in lime_explanation['true_features'].items():\n",
        "            print(f\"  {feature}: {value}\")\n",
        "\n",
        "        print(\"\\n--- Predicted Probabilities ---\")\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            prob = lime_explanation['predicted_probabilities'][i]\n",
        "            print(f\"  P(class={class_name}) = {prob:.3f}\")\n",
        "\n",
        "        print(\"\\n--- Feature Contributions (Top) ---\")\n",
        "        for feature, weight in lime_explanation['explanation_list']:\n",
        "            print(f\"  {feature:<25} --> Contribution: {weight:.3f}\")\n",
        "        print(\"====================================\")\n",
        "\n"
      ]
    }
  ]
}